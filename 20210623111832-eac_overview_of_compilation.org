#+title: EaC:Overview Of Compilation
#+roam_tags: EaC compilers books interpreter automatic-translation
#+roam_source: [[https://www.elsevier.com/books/engineering-a-compiler/cooper/978-0-12-088478-0][Engineering A Compiler]]

+ Programming language :: Formal language designed to express computations. Generally unambiguous.
+ Computer processors :: Hardware designed to execute sequences of operations. Provide a "language" of machine instructions at a low level.

The compiler translates between the a program written in a language and equivalent program in another format.
The typical compiler example transforms operations expressed in a programming language into the equivalent machine instructions.

+ Source-to-Source Translators :: Transform the source code of a language into the source code of another language.

+ Interpreter :: Transforms an executable specification and into the result of executing it.
  * EXAMPLE: Apl, Perl, Scheme are often implemented with interpreters.

+ Some languages, such as Java, include a process that both compiles and interprets. Java -> compiled to bytecode -> Interpreted by the VM.

+ The book concentrates on compilers. Compilation and Interpretation are similar and part of the material is useful for building interpreters too.

*** Why Study Compiler Construction?

    * Complex software that uses many different parts of computer science:
      + Greedy Algorithms (register allocation)
      + Heuristic search techniques (list scheduling)
      + Graph Algorithms (dead-code elimination)
      + Dynamic Programming (instruction selection)
      + Finite Automata and Push-Down automata (scanning and parsing)
      + Fixed-point algorithms (data-flow analysis)
    * Deals with complex Problems such as:
      + Dynamic Allocation
      + Synchronization
      + Naming
      + Locality
      + Memory Hierarchy Management
      + Pipeline Scheduling
        
    * Compilers play a fundamental role in the activity of computer science.

    * Represents the successful application of the theory to the practical:
      + Scanners and parsers automation comes from Formal Language Theory.
      + Similar tools are used for text searching, website filtering, word processing, command-language interpreters.
      + Type Checking and Static Analysis use results from Lattice Theory, Number Theory and other branch of mathematics.
      + Code generators use algorithms for Tree-Pattern Matching, parsing, dynamic programming and text matching to automate the selection of instructions.
    * The field has open problems to study and things to improve:
      + High-level, universal Intermediate Representation are complex.
      + Scheduling instruction is mostly done trough greedy algorithms with many layers of tie-breaking heuristics.

*** The fundamentals principle of compilation

    The most important principles:
     + The compiler must preserve the meaning of the program being compiled.
     + The compiler must improve the program in some discernible way.
       
***** Compiler Structure

    #+begin_src mermaid :file assets/images/autogenerated/two_phase_compiler_structure.png
    graph LR

      classDef default fill:#B0B0B0

      A[ ] -->|Source Program| B
      subgraph Compiler
        B[Front End] -->|IR| C[Back End]
      end
      C -->|Target Program| D[ ]

      style Compiler fill:#ffffff00
    #+end_src

    #+RESULTS:
    [[file:assets/images/autogenerated/two_phase_compiler_structure.png]]

    - Single-box model for a compiler

    - Front-end focuses on the source-language program.
    - The back-end focuses on the mapping programs to the target machine.

    The front-end uses an intermediate representation (IR) as the communication language with the back-end.
       * More than one IR may be used in a single compilation process
    The back-end then maps the IR to the target program.
       * The back-end assumes that the IR is free of error and is well-formed.
       * The compiler may make multiple passes over the IR, passing some state from one pass to another, to improve the code.
    
    + Three-phase-compiler: Adds an optimizer between the two phases that improves the IR over multiple passes.
      * An optimizer is a compiler itself, from IR to IR.

      #+begin_src mermaid :file assets/images/autogenerated/three_phase_compiler_structure.png
      graph LR

        classDef default fill:#B0B0B0

        A[ ] -->|Source Program| B
        subgraph Compiler
          B[Front End] -->|IR| O[Optimizer] -->|IR| C[Back End]
        end
        C -->|Target Program| D[ ]

        style Compiler fill:#ffffff00
      #+end_src

      #+RESULTS:
      [[file:assets/images/autogenerated/three_phase_compiler_structure.png]]
    
    Usually, the three-phases are actually divided into various passes, for example:

    #+begin_src mermaid :file assets/images/autogenerated/three_phase_compiler_passes_structure.png :width 800 height: 400
    flowchart TD

      classDef default fill:#B0B0B0

      A[ ] --> Scanner
      
      subgraph Phases
        direction LR

        subgraph FrontEnd
          direction LR
          Scanner --> Parser --> Elaboration
        end

        subgraph Optimizer
          direction LR
          O[Optimization 1] --> OO[Optimization 2] --> OOO[Optimization 3]
        end

       subgraph BackEnd
          direction LR
          BE[Inst Selection] --> BEBE[Inst Scheduling] --> BEBEBE[Reg Allocation]
       end
      end

      Elaboration --> O
      OOO --> BE
      BEBEBE --> B[ ]

      Scanner & Parser & Elaboration <--> Infrastructure
      O & OO & OOO <--> Infrastructure
      BE & BEBE & BEBEBE <--> Infrastructure

      style Phases fill:#ffffff00
      style FrontEnd fill:#ffffff00
    #+end_src

    #+RESULTS:
    [[file:assets/images/autogenerated/three_phase_compiler_passes_structure.png]]

   
   
*** Overview of Translation

      
***** The Front End

      + Determines if the input code if well formed, both syntactically and semantically. If the code is valid, an intermediate representation is created. Otherwise, errors are reported back to the user.

      1. Checking Syntax

         * Compares the program's structure against a definition of the language.
           * Mathematically, the source language is a set of strings defined by some finite rules, called a /grammar/.
      
         * Two passes:
           + Scanning :: Identifies what are distinct words (Tokens) in the input program.

             From a stream of characters a sequence of pairs \((p, s)\) are created, where \(p\) is the /part of speed/ (sentence, noun, ...) and \(s\) is the spelling (the actual string).

           + Parsing :: Matches the derived tokens against the syntax of the language to identify if it is grammatically correct.

             A derivation is created to based on the rules of the languages.
             A valid derivation may still be meaningless at the semantic level.

      2. Intermediate Representation is created from the parsed syntax.

***** The Optimizer

      The Front End is generally bound in the way in which it interprets the code and generates the IR, making the generated representation non optimal.

      The Optimizer analyzes this representation and rewrites it, where possible, to arrive at the same answer in a more efficient way.

      1. Analysis

         Determines where a transformation may be applied.

         For example, /Data-flow analysis/ reasons at compiler-time about the flow of data at runtime, solving a system of simultaneous equations to find an optimal answer.
         /Dependence analysis/, instead, uses number-theoretic tests to reasons about values that can be assumed by subscript expressions.

         For example:

        #+caption: An example of improving code. Since /a/ and /b/ never changes, /2 * b * c/ can be calculated only once outside the loop and be reused.
        |Original Code  in Context| Improved Code |
        |-------------------------+---------------|
        |[[originalcode]]             |[[improvedcode]]   |
         
     2. Transformation

        Uses the result of Analysis to rewrite the code.

        Many kind of transformation exist, such as discovering loop invariants and moving them to less frequently executed code.
       
***** The Back end

      Traverses the IR and emits code for the target machine.
      It decides and order for which the operation will execute efficiently, decides which values will reside in memory and inserts code to enforce those decisions.

      1. Instruction Selection

         Rewrites the IR operation into machine operations.

         For example the following conversion might be done: 
         
         #+name compiledcode
         #+caption: Compiled code for a <- a * 2 * b * c * d
         #+begin_center
         loadAI r_arp, @a => r_a      // load 'a'
         loadI  2         => r_2      // constant 2 into r_2
         loadAI r_arp, @b => r_b      // load 'b'
         loadAI r_arp, @c => r_c      // load 'c'
         loadAI r_arp, @d => r_d      // load 'd'
         mult r_a, r_2    => r_a      // r_a <- a * 2
         mult r_a, r_b    => r_A      // r_a <- (a * 2) * b
         mult r_a, r_c    => r_A      // r_a <- (a * 2 * b) * c
         mult r_a, r_d    => r_A      // r_a <- (a * 2 * b * c) * d
         storeAI r_a      => r_arp,@a // write r_a back to 'a'
         #+end_center

         This is a straightforward choice of operation. Each value is loaded into a register and the multiplication is executed in order.

         The instructor selector can make use of target specific special
         operations. For example, if an immediate-multiply operation was
         available, it could be avoided to load the constant 2 into a register
         or, if addition is faster than multiplication, it may be used to
         instead of the multiplication by 2.

     2. Register Allocation

        In the previous example, it was assumed that enough registers were
        available for the operations which were stored in /virtual registers/.
        It may be possible that more registers than the one available are
        needed. The register allocator maps /virtual registers/ to real
        registers on the target-machine and rewrites the code to reflect the
        decision.

        For example, it might rewrite the previous example to optimize the register usage, using three instead of six registers:

         #+name compiledcode_register_allocation
         #+caption: Compiled code for a <- a * 2 * b * c * d
         #+begin_center
         loadAI r_arp, @a => r_1      // load 'a'
         add r_1, r_1     => r_1      // r_1 <- a * 2
         loadAI r_arp, @b => r_2      // load 'b'
         mult r_1, r_2    => r_1      // r_1 <- (a * 2) * b
         loadAI r_arp, @c => r_2      // load 'c'
         mult r_1, r_2    => r_1      // r_1 <- (a * 2 * b) * c
         loadAI r_arp, @d => r_2      // load 'd'
         mult r_1, r_2    => r_1      // r_1 <- (a * 2 * b * c) * d
         storeAI r_1      => r_arp,@a // write r_a back to 'a'
         #+end_center

        This might not be the most optimized choice. For example, if any of the
        values are already in registers at those point, they should simply
        reference the already existing registers instead of minimizing register
        usage.
        
    3. Instruction Scheduling

       To produce quick code, the generator may need to reorder operations.
       The execution time of each operation may be vary, with dramatic impact on the performance of the code.

       Assume that /loadAI/ and /storeAI/ takes three cycles, /mult/ takes two cycles and all other operations takes one cycle.
       Then, the following code would take the following amount of cycles.
       
        #+name: compiledcode_register_allocation_cycles
        #+caption Cycle requirements for the register optimized code
        |------|-----|--------------------------------------------------------------|
        |Start | End |                                                                  |
        |-+-+--------------------------------------------------------------------|
        | 1 | 3 |  loadAI r_arp, @a => r_1      // load 'a'                          |
        | 4 | 4       |  add r_1, r_1     => r_1      // r_1 <- a * 2                      |
        | 5 | 7 |  loadAI r_arp, @b => r_2      // load 'b'                          |
        | 8 | 9 |  mult r_1, r_2    => r_1      // r_1 <- (a * 2) * b                |
        | 10 | 12 |  loadAI r_arp, @c => r_2      // load 'c'                          |
        | 13 | 14 |  mult r_1, r_2    => r_1      // r_1 <- (a * 2 * b) * c            |
        | 15 | 17 |  loadAI r_arp, @d => r_2      // load 'd'                          |
        | 18 | 19 |  mult r_1, r_2    => r_1      // r_1 <- (a * 2 * b * c) * d        |
        | 20 | 22 |  storeAI r_1      => r_arp,@a // write r_a back to 'a'             |
        |----+----+--------------------------------------------------------------------|

        For a total of 22 cycles.

        Many processors allow to initiate new operations while a long latency operation is executed, as long as the result of the operation is not referenced.

        Using this, the instruction scheduler can reorder operations to minimize the number of waster cycles.
        
        #+name: compiledcode_register_allocation_cycles_optimized
        #+caption Optimized cycle requirements for the register optimized code
        |-------+-----+------------------------------------------------------------|
        | Start | End |                                                            |
        |-------+-----+------------------------------------------------------------|
        |     1 |   3 | loadAI r_arp, @a => r_1      // load 'a'                   |
        |     2 |   4 | loadAI r_arp, @b => r_2      // load 'b'                   |
        |     3 |   5 | loadAI r_arp, @c => r_3      // load 'c'                   |
        |     4 |   4 | add r_1, r_1     => r_1      // r_1 <- a * 2               |
        |     5 |   6 | mult r_1, r_2    => r_1      // r_1 <- (a * 2) * b         |
        |     6 |   8 | loadAI r_arp, @d => r_2      // load 'd'                   |
        |     7 |   8 | mult r_1, r_3    => r_1      // r_1 <- (a * 2 * b) * c     |
        |     9 |  10 | mult r_1, r_2    => r_1      // r_1 <- (a * 2 * b * c) * d |
        |    11 |  13 | storeAI r_1      => r_arp,@a // write r_a back to 'a'      |
        |-------+-----+------------------------------------------------------------|

***** Interactions Among Code-Generation Components

      + Most of the hard problems come from the interaction between the various phases and problems of code generation.

        For example, instruction scheduling moves the load operation away from
        the place where they are needed for the arithmetic operations,
        increasing the time that they occupy a register.

        Similarly, the assignment to particular registers may constrain the
        operations such that they cannot be optimized, creating a possibly false dependence between two operations.
        
* Code blocks
  
         #+name: originalcode
         #+caption: Original code in Context
         #+begin_src
          b <- ...
          c <- ...
          a <- 1
          for i = 1 to n
            read d
            a <- a * 2 * b * c * d
            end
         #+end_src
         
         #+name: improvedcode
         #+caption: Improved Code
         #+begin_src
          b <- ...
          c <- ...
          a <- 1
          t <- 2 * b * c
          for i = 1 to n
            read d
            a <- a * d * t
            end
         #+end_src
